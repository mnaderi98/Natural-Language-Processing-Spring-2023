{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QITBI9Ckkf-"
   },
   "source": [
    "First find the embeddings of all words in the dataset and then pick a random word and find 10 words that are close to it.Your metric to find similarity should be **Euclidean Distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNFhnSCkn9kg"
   },
   "source": [
    "#Load Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m7gO5NaikSPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uxAUKzFLqca-"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2536\\3259244987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1465\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m \u001b[1;31m# Enable CUDA Sanitizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decomp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_add_op_to_registry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_decomposition_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOpOverload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_elementwise_meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_decomp\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;31m# populate the table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_decomp\\decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_prims\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msym_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_get_default_device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnvfuser_prims\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_nvprims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m from torch._prims_common import (\n\u001b[0;32m     18\u001b[0m     \u001b[0mcheck\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_prims\\nvfuser_prims.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_nvfuser_unary_ops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     exec(\n\u001b[0m\u001b[0;32m    156\u001b[0m         f\"\"\"\n\u001b[0;32m    157\u001b[0m \u001b[1;31m# Ensure that the nvfuser implementation exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_prims\\nvfuser_prims.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_prims\\nvfuser_prims.py\u001b[0m in \u001b[0;36m_assert_nvfuser_op_exists\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_assert_nvfuser_op_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mnvfuser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFusionDefinition\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m  \u001b[1;31m# type: ignore[import]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_join\u001b[1;34m(*path_parts)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5N_om_cl5H2"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkhOhpT_mBt8"
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(model_checkpoint, output_hidden_states = True)\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CySos-hmSjb"
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Preprocesses text input in a way that BERT can interpret.\n",
    "  \"\"\"\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1]*len(indexed_tokens)\n",
    "  # convert inputs to tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensor = torch.tensor([segments_ids])\n",
    "  return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0MPeZyzmU16"
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "  \"\"\"\n",
    "  Obtains BERT embeddings for tokens.\n",
    "  \"\"\"\n",
    "  # gradient calculation id disabled\n",
    "  with torch.no_grad():\n",
    "    # obtain hidden states\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs[2]\n",
    "  # concatenate the tensors for all layers\n",
    "  # use \"stack\" to create new dimension in tensor\n",
    "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "  # remove dimension 1, the \"batches\"\n",
    "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "  # swap dimensions 0 and 1 so we can loop over tokens\n",
    "  token_embeddings = token_embeddings.permute(1,0,2)\n",
    "  # intialized list to store embeddings\n",
    "  token_vecs = []\n",
    "  # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "  # where Y is the number of tokens in the sentence\n",
    "  # loop over tokens in sentence\n",
    "  for token in token_embeddings:\n",
    "  # \"token\" is a [12 x 768] tensor\n",
    "  # sum the vectors from the last four layers\n",
    "      token_vec = token[-1]\n",
    "      token_vecs.append(token_vec)\n",
    "  return token_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJQqZXQ1n1BB"
   },
   "source": [
    "#Load Dataset and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr4fwITmnc9o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"q1_sent_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRQ1HPCXm7QA"
   },
   "outputs": [],
   "source": [
    "def delete_hashtag_usernames(text):\n",
    "  try:\n",
    "    result = []\n",
    "    for word in text.split():\n",
    "      if word[0] not in ['@', '#']:\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "  except:\n",
    "    return ''\n",
    "\n",
    "def delete_url(text):\n",
    "  text = re.sub(r'http\\S+', '', text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_gwXR07swHQ"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "special_tokens = ['[UNK]', '[CLS]', '[SEP]']\n",
    "def remove_unuseful_tokens(tokens):\n",
    "  tokens_without_stopwords = list()\n",
    "  for token in context_sorted:\n",
    "    if token in string.punctuation:\n",
    "      continue\n",
    "    elif '#' in token:\n",
    "      continue\n",
    "    elif token in special_tokens:\n",
    "      continue\n",
    "    elif token[0] == '[':\n",
    "      continue\n",
    "    else:\n",
    "      tokens_without_stopwords.append(token)\n",
    "  return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkvxlIyonNkr"
   },
   "outputs": [],
   "source": [
    "# 1. extract all tweets from files and save them in memory base on each year.\n",
    "\n",
    "texts = df[\"text\"]\n",
    "normalized_list = []\n",
    "for text in texts:\n",
    "  new_text = delete_url(text)\n",
    "  new_text = delete_hashtag_usernames(new_text)\n",
    "  normalized_list.append(new_text)\n",
    "\n",
    "normalized_texts = pd.DataFrame(normalized_list, columns=['text'])\n",
    "tweets = normalized_texts[\"text\"]\n",
    "\n",
    "# 2. remove urls, hashtags and usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEkdyxuCmkmw"
   },
   "outputs": [],
   "source": [
    "sentences = tweets[:1000]\n",
    "from collections import OrderedDict\n",
    "context_dict = {} # key is token and values are (embeddings, count)\n",
    "result_dict1 = dict()\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "for sentence in sentences:\n",
    "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "  # make ordered dictionary to keep track of the position of each   word\n",
    "  tokens = OrderedDict()\n",
    "  # loop over tokens in sensitive sentence\n",
    "  for token in tokenized_text[1:-1]:\n",
    "    # keep track of position of word and whether it occurs multiple times\n",
    "    if token in tokens:\n",
    "      tokens[token] += 1\n",
    "    else:\n",
    "      tokens[token] = 1\n",
    "    # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "    # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "    \n",
    "    # save values\n",
    "    if token in context_dict:\n",
    "      context_dict[token]['embedding'] += token_vec\n",
    "      context_dict[token]['count'] += 1\n",
    "    else:\n",
    "      context_dict[token] = {'embedding': token_vec, 'count': 1}\n",
    "\n",
    "context_sorted = sorted(context_dict, key=lambda k: context_dict[k]['count'])\n",
    "context_sorted.reverse()\n",
    "\n",
    "tokens_without_stopwords = remove_unuseful_tokens(context_sorted)\n",
    "\n",
    "for token in tokens_without_stopwords[:1000]:\n",
    "  context_tokens.append(token)\n",
    "  context_embeddings.append(context_dict[token]['embedding'] / context_dict[token]['count'])\n",
    "  result_dict1[token] = context_dict[token]['embedding'] / context_dict[token]['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoRWJgzCoGVb"
   },
   "source": [
    "#Find 10 Nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md8pCSKVnpOu"
   },
   "outputs": [],
   "source": [
    "pdist = nn.PairwiseDistance(p=2)\n",
    "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
    "  words_cosine_similarity = dict()\n",
    "  for token in embedding_dict.keys():\n",
    "    words_cosine_similarity[token] = pdist(embedding_dict[word], embedding_dict[token]).item()\n",
    "  words_cosine_similarity = dict(sorted(words_cosine_similarity.items(), key=lambda item: item[1]))\n",
    "  return list(words_cosine_similarity.keys())[-k:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO80I89ZmyFb"
   },
   "outputs": [],
   "source": [
    "word = '' # Pick a random word from dataset\n",
    "# 2. find 10 nearest words\n",
    "top_10_nearest_words = find_k_nearest_neighbors(word, result_dict1, 10)\n",
    "\n",
    "# Print the top-10 words\n",
    "for i, word in enumerate(top_10_nearest_words):\n",
    "    print(f\"{i}- {word}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cNFhnSCkn9kg",
    "YoRWJgzCoGVb"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
