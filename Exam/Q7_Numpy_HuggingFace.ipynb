{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzWng3dudb_R"
   },
   "source": [
    "# Colab and Numpy\n",
    "\n",
    "## 1. Softmax\n",
    "If you don't remember Softmax details, you can visit here:\n",
    "https://en.wikipedia.org/wiki/Softmax_function \\\\ \\\\\n",
    "\n",
    "Write a function that computes the softmax using numpy functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1684999742951,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "Ve2vNVvleSIJ"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685000230824,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "Hx3GVD-VdP4H"
   },
   "outputs": [],
   "source": [
    "def Softmax(logits):\n",
    "    makhraj = np.sum(np.exp(logits))\n",
    "    s = np.exp(logits)/makhraj\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPZdKMfSea7S"
   },
   "source": [
    "Let's say the logits for the output layer of your neural network are the last 4 digits of your student id. Run your code above and output the softmax values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685000233479,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "cFcbzJSkeqt0",
    "outputId": "6d34e7e8-1e88-4a7b-b777-3a9b3cdf8ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.89843956e-03 6.62931706e-04 7.26992890e-01 2.67445738e-01]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "student_id = 98522076\n",
    "logits = np.array([2,0,7,6])\n",
    "soft_max_logits = Softmax(logits)\n",
    "print(soft_max_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g43HWa65e2S8"
   },
   "source": [
    "##2. Temperature\n",
    "\n",
    "Write a new function to calculate softmax with a Temperature parameter. \\\\\n",
    "\n",
    "\n",
    "![images.jpg](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1685000258585,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "mM1lTkBUeu5U"
   },
   "outputs": [],
   "source": [
    "def Softmax(logits, T = 1):\n",
    "    makhraj = np.sum(np.exp(logits/T))\n",
    "    s = np.exp(logits/T)/makhraj\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "011G-I7mfxxW"
   },
   "source": [
    "### Temperature Values\n",
    "\n",
    "Provide the softmax value from the logits above for various temperatures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc9doVC4gIOJ"
   },
   "source": [
    "#### T = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685000260251,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "u0WxzAjqfxWK",
    "outputId": "155bbc3c-f9a8-4c43-ac47-1cce1e0b1298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.89843956e-03 6.62931706e-04 7.26992890e-01 2.67445738e-01]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "soft_max_logits = Softmax(logits, 1)\n",
    "print(soft_max_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMne1YBxgJ4_"
   },
   "source": [
    "#### T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1685000272413,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "5vU3OedUgNGm",
    "outputId": "8fccf27e-52ea-4d83-8f4c-b0731551c6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20164231 0.16509076 0.33245196 0.30081497]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "soft_max_logits = Softmax(logits, 10)\n",
    "print(soft_max_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyZYKw_MgNco"
   },
   "source": [
    "#### T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1685000283474,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "nKqaK7v5gQ-x",
    "outputId": "c1e78f2a-0ae8-46e4-8d84-4c76cb66d798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24556265 0.24070018 0.25815292 0.25558425]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "soft_max_logits = Softmax(logits, 100)\n",
    "print(soft_max_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH-EwwRQgSWI"
   },
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "Explain how temperature affects logits and what it's good for?\n",
    "\n",
    "\\# YOUR EXPLANATION HERE \\#\n",
    "\n",
    "احتمال ها رو تغییر داده و اسکیل کزده مثلا در اینجا ورودی هایی که بیشترین احتمال رو داشتند رو احتمالشون رو کاهش داده و اونهایی که احتمالشون خیلی کم بوده رو بیشتر کرده و هر چی مقدار تی بیشتر بشه خروجی ها به هم نزدیک میشوند.\n",
    "\n",
    "اصلا برای طبقه بندی که قراره فقط یک جواب داشته باشیم خوب نیشت ولی میتونه برای جاهایی که میشه چند تا جواب داشت مثلا ترنسلیشن یا کوسشن انسورینگ خوب باشه.\n",
    "چون در طی آموزششون میتونیم چند تا جواب رو دنبال کنیم و درمرحله ی آخر یکی رو انتخاب کنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c68wAexvhy_P"
   },
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3881,
     "status": "ok",
     "timestamp": 1685000699290,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "wFPWF8DSvee4",
    "outputId": "7619630e-8d36-4fae-9d56-6250c754e595"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f20bed868d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12649,
     "status": "ok",
     "timestamp": 1685000713591,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "-Dh9cs9-h1dm",
    "outputId": "5f7658f0-53b4-41e9-c04f-c0f3da02dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "# install transformers library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co-D2fzOh49E"
   },
   "source": [
    "Load GPT2-Large from https://huggingface.co/gpt2-large \\\\\n",
    "Do not forget to load with it's LM head for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2147,
     "status": "ok",
     "timestamp": 1685001138486,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "njhK1l52kPvs"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyA2N1O4oenD"
   },
   "source": [
    "## 1. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrtD__B7oiz0"
   },
   "source": [
    "Language model heads at huggingface, provide us auto-regressive text generation their respective GenerationMixin class. \\\\ \n",
    "\n",
    "First of all, take a look at [generate](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) function and its arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEJrNs9urVxA"
   },
   "source": [
    "### 1.1 Greedy decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1LTxnp2sYH7"
   },
   "source": [
    "By default, this function generates with greedy decoding. To get started, please resume this text with greedy strategy with **maximum sequence length of 50** \\\\\n",
    "You can simply call model.generate(**args), all you have to do is to figure out the right arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1203,
     "status": "ok",
     "timestamp": 1685001561426,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "w9cYkBuBlVSD"
   },
   "outputs": [],
   "source": [
    "text = \"There is an exam on Thursday morning and\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13729,
     "status": "ok",
     "timestamp": 1685001576583,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "TkhHcA4trcmi",
    "outputId": "699ebed7-5ddc-4394-e4b1-86806c08b802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) There is an exam on Thursday morning and everyone should be ready for it,\" said Mr. Smith.\n",
      "\n",
      "The school and some students were sent to nearby St. Mary's Church, where parents and students can pray and receive other offerings. The prayer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) There is an exam on Thursday morning and the next day (Wednesday) we'll get back with you to check in. I know I've missed you in class. We'll do that tomorrow.\n",
      "\n",
      "\n",
      "\"The rest of the week (Friday)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) There is an exam on Thursday morning and we're going to meet again next month. We're going to get that done today.\"\n",
      "\n",
      "The plan is to spend the week before the exam reviewing documents, taking notes and writing notes to make sure the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) There is an exam on Thursday morning and you're trying to figure out the answer to that question. It may be an important question that is not yet answered.\"\n",
      "\n",
      "Lambert is also taking a second chance and will now need to perform a\n",
      "5) There is an exam on Thursday morning and you will have to show up in the room after 8am to be admitted to the hospital. The hospital will have to allow you to enter the hospital room at 9am and then you will be admitted to the\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE ###\n",
    "# generate 5 sentences using the tokenized_prompt\n",
    "greedy_output = []\n",
    "for i in range(5):\n",
    "    greedy_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    print(f\"{i + 1}) {tokenizer.batch_decode(greedy_output)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1685001615111,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "Ux7yBki3l3QE",
    "outputId": "4910a5d7-3dfd-45e2-bf67-a16236c5b0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There is an exam on Thursday morning and you will have to show up in the room after 8am to be admitted to the hospital. The hospital will have to allow you to enter the hospital room at 9am and then you will be admitted to the\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agFDBJ3PtYxV"
   },
   "source": [
    "### 1.2 Beam Search\n",
    "Take a look at [generate](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) function and add a new argument to use beam search strategy. **Beams size is 5. Limit maximum length to 50.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8010,
     "status": "ok",
     "timestamp": 1685003340227,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "jhm3UL-xmm4D",
    "outputId": "99b1c9af-f3a4-4112-8a49-237c437ee46c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE ###\n",
    "for i in range(2):\n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        top_k=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685003342489,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "LgvRE-zqnIFa",
    "outputId": "b1ad0778-3455-4d2f-b57e-90126949c6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There is an exam on Thursday morning and it will be a little bit longer,\" he said.\n",
      "\n",
      "Mr Turnbull's comments come after the Australian Electoral Commission released a list of candidates who had received the most votes from the electorate in the last election,\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybiQ-A_Lt6uQ"
   },
   "source": [
    "### 1.3 STOP REPEATING!\n",
    "As you can see, there are repeating ngrams! Let's make our generation a bit cleaneer. **Again, using beam size of 5, try not repeating ngrams of size 2. Limit maximum length to 50.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4956,
     "status": "ok",
     "timestamp": 1685003318298,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "GCgRyb1SnQYJ",
    "outputId": "64bb7d4b-a255-4095-daa1-6040d94000dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        top_k=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685003321304,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "IgXD_4r6njvk",
    "outputId": "fd33419b-776f-4fe6-f229-5f4cc8ea0eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There is an exam on Thursday morning and I will be back in a few hours to do a little bit of homework on that.\"\n",
      "\n",
      "\"I'm going to be back in a couple hours,\" he said. \"I'm going to be back\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTQVB-uyv190"
   },
   "source": [
    "### 1.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRCFSmWP13tX"
   },
   "source": [
    "Compare these three outputs and explain how we can make it better.\n",
    "\n",
    "######################\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwh_-GSju6rP"
   },
   "source": [
    "## 2. Push to hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkA-M9nQu9KT"
   },
   "source": [
    "Push your GPT2-Large to hub. Remember you have to be a member of our organization, or else we are unable to locate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "d3a0ef4fc0194d9bbcec24d30fb660e0",
      "272e5eaf3efb4c3b86275fa6aae3b81f",
      "a8a0d1d49f6145afb5f5fa91f6fe9722",
      "e4bbb3aabdfc4c1a909be661ad1a65f4",
      "43b30f0fd98b4aba92217828825711f4",
      "361be48569a94797a5945decc0b9f2d4",
      "55705d6de141456188dde76bdf875641",
      "69680ad5e4144a5fac681db622b6cd8e",
      "02ffbdbdfdef421098963a3c57ce212d",
      "6f94e4c8d2c44847b35ea45a7a79c4b0",
      "e9f1004f55464d5cb7452d332ac90574"
     ]
    },
    "executionInfo": {
     "elapsed": 29311,
     "status": "ok",
     "timestamp": 1685003768565,
     "user": {
      "displayName": "Mahdieh N",
      "userId": "04100878850335649721"
     },
     "user_tz": -210
    },
    "id": "-9jRlOj6u8PX",
    "outputId": "13cb2950-ade3-479a-8390-6ed1e87ff156"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a0ef4fc0194d9bbcec24d30fb660e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mahdieh98/Exam-Part7-GPT2-Large/commit/4f180510d610c863de98b420d9d5c2a43f9cb2dd', commit_message='Upload GPT2-Large', commit_description='', oid='4f180510d610c863de98b420d9d5c2a43f9cb2dd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
    "text = \"I'm hungry\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model.generate(**encoded_input)\n",
    "# TOKEN = \"it should be hidden\"\n",
    "tokenizer.push_to_hub(\"Exam-Part7-GPT2-Large\",  use_auth_token=TOKEN, commit_message=\"Upload GPT2-Large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tduMlyXgvuwy"
   },
   "source": [
    "# Temperature at generation (extra point) \n",
    "\n",
    "Can you explain temperature argument in generation? How to use it? Provide us a config that model have multiple choices with nearly same probability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAYJzthGvxse"
   },
   "outputs": [],
   "source": [
    "# do_sample = True\n",
    "# temperature = 100\n",
    "# top_k  = 40 # or top_p = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9ZP2yR1zo1M"
   },
   "source": [
    "What happens if T &#8594; 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM4sjO-60K1w"
   },
   "source": [
    "##########\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ffbdbdfdef421098963a3c57ce212d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "272e5eaf3efb4c3b86275fa6aae3b81f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_361be48569a94797a5945decc0b9f2d4",
      "placeholder": "​",
      "style": "IPY_MODEL_55705d6de141456188dde76bdf875641",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "361be48569a94797a5945decc0b9f2d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43b30f0fd98b4aba92217828825711f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55705d6de141456188dde76bdf875641": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69680ad5e4144a5fac681db622b6cd8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f94e4c8d2c44847b35ea45a7a79c4b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8a0d1d49f6145afb5f5fa91f6fe9722": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69680ad5e4144a5fac681db622b6cd8e",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02ffbdbdfdef421098963a3c57ce212d",
      "value": 124
     }
    },
    "d3a0ef4fc0194d9bbcec24d30fb660e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_272e5eaf3efb4c3b86275fa6aae3b81f",
       "IPY_MODEL_a8a0d1d49f6145afb5f5fa91f6fe9722",
       "IPY_MODEL_e4bbb3aabdfc4c1a909be661ad1a65f4"
      ],
      "layout": "IPY_MODEL_43b30f0fd98b4aba92217828825711f4"
     }
    },
    "e4bbb3aabdfc4c1a909be661ad1a65f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f94e4c8d2c44847b35ea45a7a79c4b0",
      "placeholder": "​",
      "style": "IPY_MODEL_e9f1004f55464d5cb7452d332ac90574",
      "value": " 124/124 [00:00&lt;00:00, 5.49kB/s]"
     }
    },
    "e9f1004f55464d5cb7452d332ac90574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
